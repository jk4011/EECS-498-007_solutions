{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Policy Gradient.ipynb","provenance":[],"authorship_tag":"ABX9TyO4d8k1bfUHyELpv0Cw8kdb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"UrYlPw1j6avI"},"source":["## Initial Setting"]},{"cell_type":"code","metadata":{"id":"7So2O97G6as_","executionInfo":{"status":"ok","timestamp":1613724869891,"user_tz":-540,"elapsed":691,"user":{"displayName":"김진혁","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7XSL5Nt087WeoH5pL7Ycx-asj8I-93Bvkq0eo=s64","userId":"17021973850832244622"}}},"source":["import torch\r\n","import torch.nn as nn\r\n","import torch.optim as optim\r\n","import torch.nn.functional as F\r\n","import torch.distributions as distributions\r\n","\r\n","import matplotlib.pyplot as plt\r\n","import numpy as np\r\n","import gym\r\n","\r\n","train_env = gym.make('CartPole-v1')\r\n","test_env = gym.make('CartPole-v1')\r\n","\r\n","SEED = 1234\r\n","\r\n","train_env.seed(SEED);\r\n","test_env.seed(SEED+1);\r\n","np.random.seed(SEED);\r\n","torch.manual_seed(SEED);"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DbfNxhAk6aqp"},"source":["## Vanila Multi Layer Perceptron"]},{"cell_type":"code","metadata":{"id":"SawLb5DP7Lt8","executionInfo":{"status":"ok","timestamp":1613724869892,"user_tz":-540,"elapsed":348,"user":{"displayName":"김진혁","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7XSL5Nt087WeoH5pL7Ycx-asj8I-93Bvkq0eo=s64","userId":"17021973850832244622"}}},"source":["class MLP(nn.Module):\r\n","    def __init__(self, input_dim, hidden_dim, output_dim, dropout = 0.5):\r\n","        super().__init__()\r\n","\r\n","        self.fc_1 = nn.Linear(input_dim, hidden_dim)\r\n","        self.fc_2 = nn.Linear(hidden_dim, output_dim)\r\n","        self.dropout = nn.Dropout(dropout)\r\n","\r\n","    def forward(self, x):\r\n","        x = self.fc_1(x)\r\n","        x = self.dropout(x)\r\n","        x = F.relu(x)\r\n","        x = self.fc_2(x)\r\n","        return x\r\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"CCicIONa6aoE","executionInfo":{"status":"ok","timestamp":1613724870558,"user_tz":-540,"elapsed":851,"user":{"displayName":"김진혁","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7XSL5Nt087WeoH5pL7Ycx-asj8I-93Bvkq0eo=s64","userId":"17021973850832244622"}}},"source":["INPUT_DIM = train_env.observation_space.shape[0]\r\n","HIDDEN_DIM = 128\r\n","OUTPUT_DIM = train_env.action_space.n\r\n","\r\n","policy = MLP(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM)\r\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"l43NaVwp6alb","executionInfo":{"status":"ok","timestamp":1613724870558,"user_tz":-540,"elapsed":696,"user":{"displayName":"김진혁","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7XSL5Nt087WeoH5pL7Ycx-asj8I-93Bvkq0eo=s64","userId":"17021973850832244622"}}},"source":["def init_weights(m):\r\n","    if type(m) == nn.Linear:\r\n","        torch.nn.init.xavier_normal_(m.weight) # init 하는 algorithm이 있음.\r\n","        m.bias.data.fill_(0)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H8dDo3hR6ai3","executionInfo":{"status":"ok","timestamp":1613724870559,"user_tz":-540,"elapsed":523,"user":{"displayName":"김진혁","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7XSL5Nt087WeoH5pL7Ycx-asj8I-93Bvkq0eo=s64","userId":"17021973850832244622"}},"outputId":"d7d4b7e9-c3f7-4054-f1c4-77df526e4147"},"source":["policy.apply(init_weights)"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MLP(\n","  (fc_1): Linear(in_features=4, out_features=128, bias=True)\n","  (fc_2): Linear(in_features=128, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"hC2Fdt2F6agc","executionInfo":{"status":"ok","timestamp":1613724871058,"user_tz":-540,"elapsed":865,"user":{"displayName":"김진혁","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7XSL5Nt087WeoH5pL7Ycx-asj8I-93Bvkq0eo=s64","userId":"17021973850832244622"}}},"source":["LEARNING_RATE = 0.01\r\n","\r\n","optimizer = optim.Adam(policy.parameters(), lr = LEARNING_RATE)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oTnazlzU6adw"},"source":["### train"]},{"cell_type":"code","metadata":{"id":"E7YYULW66abk","executionInfo":{"status":"ok","timestamp":1613724879781,"user_tz":-540,"elapsed":528,"user":{"displayName":"김진혁","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7XSL5Nt087WeoH5pL7Ycx-asj8I-93Bvkq0eo=s64","userId":"17021973850832244622"}}},"source":["def train(env, policy, optimizer, discount_factor):\r\n","    \r\n","    policy.train()\r\n","    \r\n","    log_prob_actions = []\r\n","    rewards = []\r\n","    done = False\r\n","    episode_reward = 0\r\n","\r\n","    state = env.reset()\r\n","\r\n","    while not done:\r\n","\r\n","        state = torch.FloatTensor(state).unsqueeze(0)\r\n","\r\n","        action_pred = policy(state)\r\n","        \r\n","        action_prob = F.softmax(action_pred, dim = -1)\r\n","                \r\n","        dist = distributions.Categorical(action_prob)\r\n","        action = dist.sample() # 확률값으로 sample하게 해줌\r\n","        \r\n","        log_prob_action = dist.log_prob(action) # pass : prob를 튜닝해 줌\r\n","        \r\n","        state, reward, done, _ = env.step(action.item())\r\n","\r\n","        log_prob_actions.append(log_prob_action)\r\n","        rewards.append(reward)\r\n","\r\n","        episode_reward += reward\r\n","\r\n","    log_prob_actions = torch.cat(log_prob_actions)\r\n","        \r\n","    # 실제 Q table을 만듦.\r\n","    returns = calculate_returns(rewards, discount_factor)\r\n","        \r\n","    # update\r\n","    loss = update_policy(returns, log_prob_actions, optimizer)\r\n","\r\n","    return loss, episode_reward"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"G6wnlk_v6aY7","executionInfo":{"status":"ok","timestamp":1613724881664,"user_tz":-540,"elapsed":761,"user":{"displayName":"김진혁","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7XSL5Nt087WeoH5pL7Ycx-asj8I-93Bvkq0eo=s64","userId":"17021973850832244622"}}},"source":["def calculate_returns(rewards, discount_factor, normalize = True):\r\n","    \"\"\"apply discount factor and\r\n","    squeeze reward into regular distribution is is_normalize \"\"\"\r\n","    returns = []\r\n","    R = 0\r\n","    \r\n","    for r in reversed(rewards):\r\n","        R = r + R * discount_factor\r\n","        returns.insert(0, R)\r\n","        \r\n","    returns = torch.tensor(returns)\r\n","    \r\n","    if normalize:\r\n","        returns = (returns - returns.mean()) / returns.std()\r\n","        \r\n","    return returns"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZiJa2VaP6aWV","executionInfo":{"status":"ok","timestamp":1613724882743,"user_tz":-540,"elapsed":536,"user":{"displayName":"김진혁","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7XSL5Nt087WeoH5pL7Ycx-asj8I-93Bvkq0eo=s64","userId":"17021973850832244622"}}},"source":["def update_policy(returns, log_prob_actions, optimizer):\r\n","    \r\n","    returns = returns.detach()\r\n","    \r\n","    loss = - (returns * log_prob_actions).sum()\r\n","    \r\n","    # set grad to zero\r\n","    optimizer.zero_grad()\r\n","    \r\n","    loss.backward() # requires_grad=True 인 para들의 미분 값을 구함. \r\n","    \r\n","    # perform one optimizer step\r\n","    optimizer.step()\r\n","    \r\n","    return loss.item()"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"xSg0h4lKf9M_","executionInfo":{"status":"ok","timestamp":1613724901422,"user_tz":-540,"elapsed":806,"user":{"displayName":"김진혁","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7XSL5Nt087WeoH5pL7Ycx-asj8I-93Bvkq0eo=s64","userId":"17021973850832244622"}}},"source":["def evaluate(env, policy):\r\n","    \r\n","    policy.eval() # # set the policy to evaluate mode\r\n","    \r\n","    done = False\r\n","    episode_reward = 0\r\n","\r\n","    state = env.reset()\r\n","\r\n","    while not done:\r\n","        \r\n","        state = torch.FloatTensor(state).unsqueeze(0)\r\n","        \r\n","        with torch.no_grad(): # disabled gradient calculation\r\n","        \r\n","            action_pred = policy(state)\r\n","        \r\n","            action_prob = F.softmax(action_pred, dim = -1)\r\n","                            \r\n","        action = torch.argmax(action_prob, dim = -1)\r\n","            \r\n","        state, reward, done, _ = env.step(action.item())\r\n","\r\n","        episode_reward += reward\r\n","        \r\n","    return episode_reward"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"MM7q1_D86aM5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613724934748,"user_tz":-540,"elapsed":20612,"user":{"displayName":"김진혁","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7XSL5Nt087WeoH5pL7Ycx-asj8I-93Bvkq0eo=s64","userId":"17021973850832244622"}},"outputId":"47a75c92-44f4-45b0-c00c-c79fe2addf0b"},"source":["MAX_EPISODES = 500\r\n","DISCOUNT_FACTOR = 0.99\r\n","N_TRIALS = 25\r\n","REWARD_THRESHOLD = 475\r\n","PRINT_EVERY = 10\r\n","\r\n","train_rewards = []\r\n","test_rewards = []\r\n","\r\n","for episode in range(1, MAX_EPISODES):\r\n","\r\n","    loss, train_reward = train(train_env, policy, optimizer, DISCOUNT_FACTOR)\r\n","\r\n","    test_reward = evaluate(test_env, policy)\r\n","\r\n","    train_rewards.append(train_reward)\r\n","    test_rewards.append(test_reward)\r\n","\r\n","    mean_train_rewards = np.mean(train_rewards[-N_TRIALS:])\r\n","    mean_test_rewards = np.mean(test_rewards[-N_TRIALS:])\r\n","\r\n","    if episode % PRINT_EVERY == 0:\r\n","\r\n","        print(f'| Episode: {episode:3} | Mean Train Rewards: {mean_train_rewards:5.1f} | Mean Test Rewards: {mean_test_rewards:5.1f} |')\r\n","\r\n","    if mean_test_rewards >= REWARD_THRESHOLD:\r\n","\r\n","        print(f'Reached reward threshold in {episode} episodes')\r\n","        \r\n","        break"],"execution_count":17,"outputs":[{"output_type":"stream","text":["| Episode:  10 | Mean Train Rewards:  26.3 | Mean Test Rewards:  12.5 |\n","| Episode:  20 | Mean Train Rewards:  35.3 | Mean Test Rewards:  39.0 |\n","| Episode:  30 | Mean Train Rewards:  41.8 | Mean Test Rewards:  48.2 |\n","| Episode:  40 | Mean Train Rewards:  56.0 | Mean Test Rewards:  75.0 |\n","| Episode:  50 | Mean Train Rewards:  63.2 | Mean Test Rewards:  97.9 |\n","| Episode:  60 | Mean Train Rewards:  70.5 | Mean Test Rewards: 161.2 |\n","| Episode:  70 | Mean Train Rewards:  76.3 | Mean Test Rewards: 202.8 |\n","| Episode:  80 | Mean Train Rewards:  80.9 | Mean Test Rewards: 229.1 |\n","| Episode:  90 | Mean Train Rewards:  97.1 | Mean Test Rewards: 293.7 |\n","| Episode: 100 | Mean Train Rewards: 122.2 | Mean Test Rewards: 238.6 |\n","| Episode: 110 | Mean Train Rewards: 175.4 | Mean Test Rewards: 291.9 |\n","| Episode: 120 | Mean Train Rewards: 253.7 | Mean Test Rewards: 361.5 |\n","| Episode: 130 | Mean Train Rewards: 347.1 | Mean Test Rewards: 444.6 |\n","| Episode: 140 | Mean Train Rewards: 271.5 | Mean Test Rewards: 311.3 |\n","| Episode: 150 | Mean Train Rewards: 179.9 | Mean Test Rewards: 185.8 |\n","| Episode: 160 | Mean Train Rewards: 185.7 | Mean Test Rewards: 221.9 |\n","| Episode: 170 | Mean Train Rewards: 332.6 | Mean Test Rewards: 365.8 |\n","| Episode: 180 | Mean Train Rewards: 438.3 | Mean Test Rewards: 478.1 |\n","Reached reward threshold in 180 episodes\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oZhqNP6B6aKn","executionInfo":{"status":"ok","timestamp":1613724884015,"user_tz":-540,"elapsed":350,"user":{"displayName":"김진혁","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7XSL5Nt087WeoH5pL7Ycx-asj8I-93Bvkq0eo=s64","userId":"17021973850832244622"}}},"source":[""],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"CjOVq4G56aIJ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4RxVOFms6Sl7"},"source":[""],"execution_count":null,"outputs":[]}]}