{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pytorch_basic_operation.ipynb","private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNDRMgXef7cGRsEtpG9Skla"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"sn7N-XXH53ZW"},"source":["import torch\r\n","\r\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\n","my_tensor = torch.tensor([[1,2,3], [4,5,6]], # 2*3\r\n","                         dtype=torch.float, \r\n","                         device=device,  # device=\"cpu\" : gpu 사용 안함\r\n","                         requires_grad=True) # auto_grad - backpropagation에 사용가능"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-_BaXmcz57fl"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h_XHxnvf6Cd0"},"source":["print(my_tensor.dtype, \r\n","      my_tensor.device, \r\n","      my_tensor.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LCZnoUAa6x-n"},"source":["x= torch.tensor([[1,2,3], [4,5,6]], # 2*3\r\n","                         dtype=torch.float, \r\n","                         device=device,  # device=\"cpu\" : gpu 사용 안함\r\n","                         requires_grad=True) # auto_grad - backpropagation에 사용가능\r\n","\r\n","# x = torch.empty(size = (3,3)) #uninitialized\r\n","# x = torch.empty(size = (1,5)).normal_(mean=0, std=1) # normal distribution\r\n","# x = torch.empty(size = (1,5)).uniform_(0,1) # uniform distribution\r\n","# x = torch.zeros((3,3)) # size 생략\r\n","# x = torch.rand((3,3))\r\n","# x = torch.ones((3,3))\r\n","# x = torch.eye(5, 5) # identity matrix\r\n","# x = torch.arange(start=0, end=5, step=1)\r\n","# x = torch.linspace(start=0.1, end=1, steps=10) # 10개로 나눔\r\n","# x = torch.diag(torch.ones(3) * 2) # 1d -> 2d\r\n","print(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BMii6bO16yL4"},"source":["tensor = torch.arange(4) # [0,1,2,3]\r\n","print(\r\n","    tensor.bool())\r\n","    # tensor.short()) # int 16\r\n","    # tensor.long())  # int 64 (Important)\r\n","    # tensor.half())  # float 16\r\n","    # tensor.float()) # float 32 (Important)\r\n","    # tensor.double())# float 64"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rpxODOS-6yfO"},"source":["import numpy as np\r\n","np_array = np.zeros((5, 5))\r\n","tensor = torch.from_numpy(np_array)\r\n","np_array_back = tensor.numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eYAchlacBCGb"},"source":["# input\r\n","x = torch.tensor([1,2,3])\r\n","y = torch.tensor([9,8,7])\r\n","\r\n","# Add\r\n","z1 = torch.empty(3)\r\n","torch.add(x, y, out=z1)\r\n","x + y\r\n","\r\n","# Subtraction\r\n","z = x - y \r\n","\r\n","# Division\r\n","z = torch.true_divide(x, y)\r\n","\r\n","# inplace operations\r\n","t = torch.zeros(3)\r\n","t.add_(x) #_ means inplace operation\r\n","t += x # t = t + x is not inplace : create copy\r\n","\r\n","# Exponentiation\r\n","z = x.pow(2) # power of 2\r\n","z = x ** 2\r\n","\r\n","# Simple comparison\r\n","z = x > 0 # tensor([True, True, True])\r\n","\r\n","# Matrix Multiplication\r\n","x1 = torch.rand((2, 5))\r\n","x2 = torch.rand((5, 3))\r\n","\r\n","x3 = torch.mm(x1, x2) # 2x3\r\n","x3 = x1.mm(x2)\r\n","\r\n","# Matrix eponentiation\r\n","matrix_exp = torch.rand(5, 5)\r\n","print(matrix_exp.matrix_power(3)) # M^3 \r\n","\r\n","# Element wise multiplication\r\n","z = x * y\r\n","print(z)\r\n","\r\n","# dot product\r\n","z = torch.dot(x, y)\r\n","print(z)\r\n","\r\n","# Batch Matrix Multiplication\r\n","batch = 32\r\n","n = 10\r\n","m = 20\r\n","p = 30\r\n","\r\n","tensor1 = torch.rand((batch, n, m)) # 3d matrix\r\n","tensor2 = torch.rand((batch, m, p))\r\n","out_bmm = torch.bmm(tensor1, tensor2) # (batch, n, p)\r\n","\r\n","# Example of Broadcasting\r\n","x1 = torch.rand((5,5))\r\n","x2 = torch.rand((1,5))\r\n","\r\n","z = x1 - x2 # x2가 (5,5) 로 확장됨 : broadcasting\r\n","z = x1 ** x2\r\n","\r\n","#Other usefule tensor operations\r\n","sum_x = torch.sum(x, dim=0) # output의 dimention이 0\r\n","values, indices = torch.max(x, dim=0) # x.max(dim=0)\r\n","values, indices = torch.min(x, dim=0) \r\n","abs_x = torch.abs(x)\r\n","z = torch.argmax(x, dim=0)\r\n","z = torch.argmin(x, dim=0)\r\n","mean_x = torch.mean(x.float(), dim=0) # mean only get float matrix\r\n","z = torch.eq(x, y) # element wise ==\r\n","print(y)\r\n","\r\n","# Sort\r\n","sorted_y, indices = torch.sort(y, dim=0, descending=False) # 0차원 값 : 숫자끼리 비교\r\n","print(sorted_y, indices)\r\n","\r\n","z = torch.clamp(x, min=0, max=10) # all less then 0, becomes 0. otherwise over 10, becomes 10\r\n","\r\n","x = torch.tensor([1,0,1,1,1], dtype=torch.bool)\r\n","z = torch.any(x) # return true when any of x is true\r\n","z = torch.all(x) # return true when all of x is true\r\n","\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CEvZQmCdA-tH"},"source":["## Tenser Indexing"]},{"cell_type":"code","metadata":{"id":"xB4nyjA_jOlL"},"source":["\r\n","# State in []\r\n","x = torch.arange(10)\r\n","print(x[(x<2) | (x>8)])\r\n","print(x[x.remainder(2) == 0])\r\n","\r\n","# Useful operations\r\n","print(torch.where(x > 5, x, x*2)) # a ? b: c 랑 같음\r\n","print(torch.tensor([[0,0,1],[2,2,4]]).unique())\r\n","print(x.ndimension()) # x의 dimention\r\n","print(x.numel()) # number of element\r\n","\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ym_2ZXFKjPlU"},"source":["## Tensor Reshape"]},{"cell_type":"code","metadata":{"id":"qggwgCCOSDdy"},"source":["# Reshape\r\n","x = torch.arange(9)\r\n","x_3x3 = x.view(3,3) # matrix가 continuous 하게 저장되어 있을 때만 처리가능, 빠름\r\n","x_3x3 = x.reshape(3,3) # matrix가 sparse하게 저장되어 있어도 처리가능, 느림\r\n","\r\n","# view function restriction\r\n","y = x_3x3.t() # transpose -> continuous한 memory가 아님\r\n","# print(y.view(9)) # contiguous하지 않으면 view 쓸 때 err\r\n","y.contiguous().view(9) # contiguous하게 만들어주고 view쓰면 err X.\r\n","\r\n","# Concatenate\r\n","x1 = torch.rand((2, 5))\r\n","x2 = torch.rand((2, 5))\r\n","torch.cat((x1, x2), dim=0).shape # 4x5 \r\n","torch.cat((x1, x2), dim=1).shape # 2x10 , 요거 2개 차이가 뭘까?\r\n","\r\n","# -1\r\n","batch = 2\r\n","x = torch.rand((batch, 2, 5))\r\n","z = x.view(batch, -1)\r\n","\r\n","z = x.permute(0, 2, 1) # 0은 그대로, 1<->2 dimension 바뀜 -> batch는 안 건들이고 transpose\r\n","\r\n","x = torch.arange(10)\r\n","x.unsqueeze(dim=0) # 1 x 10\r\n","x.unsqueeze(dim=1) # 10x 1\r\n","\r\n","x = torch.arange(10).unsqueeze(0).unsqueeze(1) # 1x1x10\r\n","\r\n","z = x.squeeze(dim=1) # 1x10"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WEeXMdnQSOIs"},"source":[""]}]}